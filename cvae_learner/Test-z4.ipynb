{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Workspace problem with several narrow gaps\n",
      "\n",
      "import tensorflow as tf\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.patches as patches\n",
      "import matplotlib.gridspec as gridspec\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "import os\n",
      "import csv\n",
      "from random import randint, random\n",
      "import time\n",
      "\n",
      "# (restrict tensorflow memory growth)\n",
      "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
      "config = tf.ConfigProto()\n",
      "config.gpu_options.allow_growth=True"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# neural network parameters\n",
      "mb_size = 256\n",
      "h_Q_dim = 512\n",
      "h_P_dim = 512\n",
      "\n",
      "c = 0\n",
      "# learning rate\n",
      "lr = 1e-4\n",
      "\n",
      "# problem dimensions\n",
      "dim = 7\n",
      "dataElements = dim*3 + 32 # sample (7D), init (7D), goal (7D), cond (16 + 16 (table + box) points) //total = 53\n",
      "\n",
      "z_dim = 6 # latent\n",
      "X_dim = dim # samples\n",
      "y_dim = dim # reconstruction of the original point\n",
      "c_dim = dataElements - dim # dimension of conditioning variable"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define networks\n",
      "tf.reset_default_graph()\n",
      "X = tf.placeholder(tf.float32, shape=[None, X_dim])\n",
      "c = tf.placeholder(tf.float32, shape=[None, 46])\n",
      "    \n",
      "# Q\n",
      "inputs_Q = tf.concat(axis=1, values=[X,c])\n",
      "\n",
      "dense_Q1 = tf.layers.dense(inputs=inputs_Q, units=h_Q_dim, activation=tf.nn.relu)\n",
      "dropout_Q1 = tf.layers.dropout(inputs=dense_Q1, rate=0.5)\n",
      "dense_Q2 = tf.layers.dense(inputs=dropout_Q1, units=h_Q_dim, activation=tf.nn.relu)\n",
      "\n",
      "z_mu = tf.layers.dense(inputs=dense_Q2, units=z_dim) # output here is z_mu\n",
      "z_logvar = tf.layers.dense(inputs=dense_Q2, units=z_dim) # output here is z_logvar\n",
      "\n",
      "# P\n",
      "eps = tf.random_normal(shape=tf.shape(z_mu))\n",
      "z = z_mu + tf.exp(z_logvar / 2) * eps\n",
      "inputs_P = tf.concat(axis=1, values=[z,c])\n",
      "\n",
      "dense_P1 = tf.layers.dense(inputs=inputs_P, units=h_P_dim, activation=tf.nn.relu)\n",
      "dropout_P1 = tf.layers.dropout(inputs=dense_P1, rate=0.5)\n",
      "dense_P2 = tf.layers.dense(inputs=dropout_P1, units=h_P_dim, activation=tf.nn.relu)\n",
      "\n",
      "y = tf.layers.dense(inputs=dense_P2, units=X_dim) # fix to also output y\n",
      "\n",
      "# training\n",
      "########### comment in the one with 0 weight and uncomment the other ###########\n",
      "w = [[1, 1, 1, 1, 1, 1, 1]];\n",
      "# w = [[1, 1, 1, 0, 0, 0]];\n",
      "recon_loss = tf.losses.mean_squared_error(labels=X, predictions=y, weights=w)\n",
      "\n",
      "# TODO: fix loss function for angles going around\n",
      "kl_loss = 10**-4 * 2 * tf.reduce_sum(tf.exp(z_logvar) + z_mu**2 - 1. - z_logvar, 1)\n",
      "\n",
      "cvae_loss = tf.reduce_mean(kl_loss + recon_loss)\n",
      "\n",
      "train_step = tf.train.AdamOptimizer(lr).minimize(cvae_loss)\n",
      "\n",
      "sess = tf.Session(config=config)\n",
      "sess.run(tf.global_variables_initializer())\n",
      "it = 0;"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "saver = tf.train.Saver()\n",
      "path_ = os.getcwd() + \"/checkpoints_z6_/model.ckpt\"\n",
      "print(\"path = \",path_)\n",
      "# print(\"numTrain = \",numTrain)\n",
      "try:\n",
      "    saver.restore(sess, path_)\n",
      "    print(\"Model Restored!!\")\n",
      "except Exception as e:\n",
      "    print(\"Could not restore checkpoint!\")\n",
      "    print(e)\n",
      "print(c,z)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('path = ', '/home/vernwalrahul/projects/LearningRoadmaps/cvae_learner/checkpoints_z6_/model.ckpt')\n",
        "Could not restore checkpoint!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Unsuccessful TensorSliceReader constructor: Failed to get matching files on /home/vernwalrahul/projects/LearningRoadmaps/cvae_learner/checkpoints_z6_/model.ckpt: Not found: /home/vernwalrahul/projects/LearningRoadmaps/cvae_learner/checkpoints_z6_; No such file or directory\n",
        "\t [[Node: save_3/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_3/Const_0_0, save_3/RestoreV2/tensor_names, save_3/RestoreV2/shape_and_slices)]]\n",
        "\n",
        "Caused by op u'save_3/RestoreV2', defined at:\n",
        "  File \"<string>\", line 1, in <module>\n",
        "  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/kernelapp.py\", line 468, in main\n",
        "    app.start()\n",
        "  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/kernelapp.py\", line 458, in start\n",
        "    ioloop.IOLoop.instance().start()\n",
        "  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 160, in start\n",
        "    super(ZMQIOLoop, self).start()\n",
        "  File \"/usr/lib/python2.7/dist-packages/tornado/ioloop.py\", line 672, in start\n",
        "    self._handlers[fd](fd, events)\n",
        "  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 302, in wrapped\n",
        "    ret = fn(*args, **kwargs)\n",
        "  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 433, in _handle_events\n",
        "    self._handle_recv()\n",
        "  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 465, in _handle_recv\n",
        "    self._run_callback(callback, msg)\n",
        "  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 407, in _run_callback\n",
        "    callback(*args, **kwargs)\n",
        "  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 302, in wrapped\n",
        "    ret = fn(*args, **kwargs)\n",
        "  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py\", line 279, in dispatcher\n",
        "    return self.dispatch_shell(stream, msg)\n",
        "  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py\", line 247, in dispatch_shell\n",
        "    handler(stream, idents, msg)\n",
        "  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py\", line 396, in execute_request\n",
        "    shell.run_cell(code, store_history=store_history, silent=silent)\n",
        "  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2660, in run_cell\n",
        "    interactivity=interactivity, compiler=compiler)\n",
        "  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2764, in run_ast_nodes\n",
        "    if self.run_code(code):\n",
        "  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2820, in run_code\n",
        "    exec code_obj in self.user_global_ns, self.user_ns\n",
        "  File \"<ipython-input-45-7e3fca4ba372>\", line 1, in <module>\n",
        "    saver = tf.train.Saver()\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1338, in __init__\n",
        "    self.build()\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1347, in build\n",
        "    self._build(self._filename, build_save=True, build_restore=True)\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1384, in _build\n",
        "    build_save=build_save, build_restore=build_restore)\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 835, in _build_internal\n",
        "    restore_sequentially, reshape)\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 472, in _AddRestoreOps\n",
        "    restore_sequentially)\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 886, in bulk_restore\n",
        "    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1463, in restore_v2\n",
        "    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
        "    op_def=op_def)\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n",
        "    op_def=op_def)\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n",
        "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
        "\n",
        "InvalidArgumentError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to get matching files on /home/vernwalrahul/projects/LearningRoadmaps/cvae_learner/checkpoints_z6_/model.ckpt: Not found: /home/vernwalrahul/projects/LearningRoadmaps/cvae_learner/checkpoints_z6_; No such file or directory\n",
        "\t [[Node: save_3/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_3/Const_0_0, save_3/RestoreV2/tensor_names, save_3/RestoreV2/shape_and_slices)]]\n",
        "\n",
        "(<tf.Tensor 'Placeholder_1:0' shape=(?, 46) dtype=float32>, <tf.Tensor 'add:0' shape=(?, 6) dtype=float32>)\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create test_conditions from data dir\n",
      "print(c,z)\n",
      "def create_samples(G, directory):\n",
      "    start = np.loadtxt(directory+\"/start_node.txt\")\n",
      "    goal = np.loadtxt(directory+\"/goal_node.txt\")\n",
      "    cond = np.loadtxt(directory+\"/conditions.txt\")\n",
      "    cond = np.array([cond[23]])\n",
      "    def state_to_numpy(state):\n",
      "        strlist = state.split()\n",
      "        val_list = [float(s) for s in strlist]\n",
      "        return np.array(val_list)\n",
      "    \n",
      "    # cond = cond.split(\",\")\n",
      "    path_nodes = []\n",
      "    i = 0\n",
      "    c_samples = []\n",
      "    with open(directory + \"/path_nodes.txt\", 'r') as file:\n",
      "        lines  = file.readlines()\n",
      "        for line in lines:\n",
      "            line = line.strip('\\n')\n",
      "#             print(line)\n",
      "#             print(\"\\n\\n\")\n",
      "            if(not line == '-1'):\n",
      "                s = state_to_numpy(G.node[str(int(start[i]))]['state'])\n",
      "                g = state_to_numpy(G.node[str(int(goal[i]))]['state'])\n",
      "                path_nodes = str(line).split(\",\")\n",
      "                # print(path_nodes)\n",
      "                for path_node in path_nodes:\n",
      "                    node_conf = state_to_numpy(G.node[path_node]['state'])\n",
      "                    curr_node = np.array([])\n",
      "                    # print(\"Data = \",node_conf, s, g, cond)\n",
      "#                     print(\"\\n\")\n",
      "                    curr_node = np.concatenate((s, g, cond))\n",
      "                    c_samples.append(curr_node)\n",
      "    return np.array(c_samples)\n",
      "print(\"c, z = \",c, z)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(<tf.Tensor 'Placeholder_1:0' shape=(?, 15) dtype=float32>, <tf.Tensor 'add:0' shape=(?, 6) dtype=float32>)\n",
        "('c, z = ', <tf.Tensor 'Placeholder_1:0' shape=(?, 15) dtype=float32>, <tf.Tensor 'add:0' shape=(?, 6) dtype=float32>)\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_viz = 300\n",
      "c_final = []\n",
      "print(\"c, z = \", c, z)\n",
      "import networkx as nx\n",
      "from random import choice\n",
      "G = nx.read_graphml(\"graphs/modified_graph_30000.graphml\")\n",
      "\n",
      "dire = [\"test_data_9June/T1/2\", \"test_data_9June/T1/4\", \"test_data_9June/T2/2\", \"test_data_9June/T2/4\", \"test_data_9June/T2/10\", \"test_data_9June/T2/12\"]\n",
      "dire = [\"test_data_9June/T2/14\"]\n",
      "\n",
      "for directory in dire:\n",
      "    c_final = []\n",
      "    c_samples = np.float32(create_samples(G, directory))\n",
      "\n",
      "    for i in range(num_viz):\n",
      "        c_final.append(choice(c_samples))\n",
      "    z_final = np.random.randn(num_viz,z_dim)\n",
      "\n",
      "    c_final = np.array(c_final)\n",
      "    print(\"c_final_dim = \",c_final.shape)\n",
      "    print(\"z_dim = \",z_final.shape)\n",
      "    # directly sample from the latent space (preferred, what we will use in the end)\n",
      "    y_viz, z_viz = sess.run([y, z], feed_dict={z: z_final, c: c_final})\n",
      "\n",
      "    print(y_viz)\n",
      "    np.savetxt(directory+\"/output_samples_\"+str(z_dim)+\".txt\", y_viz, delimiter=\" \", fmt=\"%s\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('c, z = ', <tf.Tensor 'Placeholder_1:0' shape=(?, 15) dtype=float32>, <tf.Tensor 'add:0' shape=(?, 6) dtype=float32>)\n",
        "('c_final_dim = ', (300, 15))"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('z_dim = ', (300, 6))\n",
        "[[ 0.1265341   0.3277399   0.5037297  ...  0.3840124  -0.12456707\n",
        "  -0.22958562]\n",
        " [ 0.20942515  0.41979215  0.42505044 ...  0.3443595  -0.0780517\n",
        "  -0.43975168]\n",
        " [ 0.06294712  0.1314317   0.5667293  ...  0.31180882 -0.07286814\n",
        "  -0.33428693]\n",
        " ...\n",
        " [-0.16470124  0.29858455  0.6699426  ...  0.41457695 -0.1955016\n",
        "  -0.4444733 ]\n",
        " [ 0.1709292   0.2808112   0.5627403  ...  0.3262     -0.1192623\n",
        "  -0.27128315]\n",
        " [ 0.02502181  0.18895271  0.5355797  ...  0.37466785 -0.16741616\n",
        "  -0.20954597]]\n"
       ]
      }
     ],
     "prompt_number": 28
    }
   ],
   "metadata": {}
  }
 ]
}